{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP4LBCaq86jYYznnJQuV3oZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/786aafreen/PythonJenkins/blob/main/Summerization_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LugtD3AL44zu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7qIVTigE4635"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google's T5 Abstract Summarization Tutorial\n",
        "## Overview ---------downloaded on 11-Feb-22 from https://www.udemy.com/course/pytorch-deep-learning-hero/learn/lecture/24607924#overview chapter 39\n",
        "\n",
        "T5(Text-to-Text Transfer Transformer) is a model for transfer learning that takes text as input and outputs text, since it is text-to-text.\n",
        "\n",
        "Examples of text-to-text are\n",
        "1. translation\n",
        "2. question and answering\n",
        "3. classification\n",
        "4. summarization\n",
        "\n",
        "\n",
        "Mainly, there are two types of summarization: extractive summarization, in which the original document is excerpted, and generative summarization, in which the original text is conceptually abstracted and rewritten in a different way.\n",
        "In general, abstract summarization is more difficult.\n",
        "\n",
        "In this lesson, we will use T5 for abstract summarization.\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA3AAAAEqCAYAAABUYLJvAAAgAElEQVR4Aey9ZXRU2bqoff9895x77tnSbtCNS9xDiOHu7u7u3kB3Q+PWQOMQIAlxd9dKqipS8QR3CK1099mnz7577+cbq5JABRJIINAE3oxRg5Blcz3zXbXms9655vxfyI8QEAJCQAgIASEgBISAEBACQkAINAoC/"
      ],
      "metadata": {
        "id": "bAyQv3m-5Gzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "oJ5MOx-C5HSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "qJp7yCb55hRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "id": "utC7XJOW5o5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5TokenizerFast, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "GIHf0vkr5rkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfWSHhsq54A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "aruW3aO_5yV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "OOsj6O2S50j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "0n_ikY-6548n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "JsoX7Uk_56JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "oliuVFAj6ZOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "fCq2947aE2Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Data"
      ],
      "metadata": {
        "id": "LkW_0D8p6jpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/news_summary (2).csv', encoding='latin')"
      ],
      "metadata": {
        "id": "OToTU19l7J7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.ctext.iat[0] #----- what is the use of .iat"
      ],
      "metadata": {
        "id": "N4PoQ4TD7oA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text.iat[0]"
      ],
      "metadata": {
        "id": "NDrFdXYe7t5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "lI6HNMnz7xd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['text', 'ctext']]"
      ],
      "metadata": {
        "id": "mGjF3maC7zcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.index)"
      ],
      "metadata": {
        "id": "Mg66VaVJ71Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(100)"
      ],
      "metadata": {
        "id": "lms2B6Zw73Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "jX0to70uFVcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8 #------what is train size \n",
        "\n",
        "train_dataset = df.sample(frac=train_size, random_state=0)\n",
        "val_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True) #try to insert index into dataframe columns. This resets the index to the default integer index."
      ],
      "metadata": {
        "id": "hs_3GnBmFZON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset.index)"
      ],
      "metadata": {
        "id": "ivtseq2rFdDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_dataset.index)"
      ],
      "metadata": {
        "id": "ePERtCbHFft0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------what this function does ?\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.summ_len = summ_len\n",
        "    self.text = self.data.text\n",
        "    self.ctext = self.data.ctext\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    ctext = str(self.ctext[index])\n",
        "    ctext = ' '.join(ctext.split())\n",
        "\n",
        "    text = str(self.text[index])\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    source = self.tokenizer.batch_encode_plus([ctext], max_length=self.source_len, pad_to_max_length=True, return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([text], max_length=self.summ_len, pad_to_max_length=True, return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze() #The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids' : source_ids,\n",
        "        'source_mask' : source_mask,\n",
        "        'target_ids' : target_ids,\n",
        "        'target_mask' : target_mask\n",
        "    }"
      ],
      "metadata": {
        "id": "lZqdHyJOHXra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------what is the role of tokenizer ,T5TokenizerFast\n",
        "tokenizer = T5TokenizerFast.from_pretrained('t5-small')"
      ],
      "metadata": {
        "id": "McjhgU-DHaiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_encode_plus(['I can do it better'], max_length=32, pad_to_max_length=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "ULqLemosHd2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------what does CustomDataset return \n",
        "training_set = CustomDataset(train_dataset, tokenizer, 512, 150)\n",
        "val_set = CustomDataset(val_dataset, tokenizer, 512, 150)"
      ],
      "metadata": {
        "id": "nnplym26HjuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {\n",
        "    'batch_size' : 2,\n",
        "    'shuffle' : True,\n",
        "    'num_workers' : 0\n",
        "}"
      ],
      "metadata": {
        "id": "K1hqttwjHmG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_params = {\n",
        "    'batch_size' : 2,\n",
        "    'shuffle' : False,\n",
        "    'num_workers' : 0\n",
        "}"
      ],
      "metadata": {
        "id": "lSDsfylcHowD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)"
      ],
      "metadata": {
        "id": "DQMV1k3YHrEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in training_loader:\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "id": "lCeANfu2HtKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "rZnQIRGdHv71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "  model.train()\n",
        "  for i, data in enumerate(loader, 0):\n",
        "    y = data['target_ids'].to(device, dtype=torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    print( y_ids)\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id]= -100\n",
        "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "    mask = data['source_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    if i % 10  == 0:\n",
        "      wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "    if i % 500 == 0:\n",
        "      print(f'Epoch:{epoch+1}, Loss:{loss.item()}')\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "Vajes1hzHzw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(loader, 0):\n",
        "      y = data['target_ids'].to(device, dtype=torch.long)\n",
        "      ids = data['source_ids'].to(device, dtype=torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask,\n",
        "          max_length = 150, \n",
        "          num_beams = 2,\n",
        "          repetition_penalty = 2.5,\n",
        "          length_penalty = 1.0,\n",
        "          early_stopping = True\n",
        "      )\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      actuals.extend(target)\n",
        "\n",
        "      if i % 100 == 0:\n",
        "        print(f'Completed: {i}')\n",
        "  return predictions, actuals"
      ],
      "metadata": {
        "id": "aOS00EsJH1l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  wandb.init(project=\"transformers_tutorials_summarization\")\n",
        "\n",
        "  config = wandb.config\n",
        "  config.TRAIN_BATCH_SIZE = 2\n",
        "  config.VALID_BATCH_SIZE = 2\n",
        "  config.TRAIN_EPOCHS = 2\n",
        "  config.VAL_EPOCHS = 1\n",
        "  config.LEARNING_RATE = 1e-4\n",
        "  config.SEED = 42\n",
        "  config.MAX_LEN = 512\n",
        "  config.SUMMARY_LEN = 150\n",
        "\n",
        "  # Reproductivity\n",
        "  torch.manual_seed(config.SEED)\n",
        "  np.random.seed(config.SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "  wandb.watch(model, log=\"all\")\n",
        "\n",
        "  for epoch in range(config.TRAIN_EPOCHS):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "  for epoch in range(config.VAL_EPOCHS):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions, 'Actual Text':actuals})\n",
        "    \n",
        "    final_df.to_csv('gdrive/My Drive/predictions.csv')\n",
        "    print('Ouput files generated for review')"
      ],
      "metadata": {
        "id": "uwBpm3WyH78z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "WmqLeNmAH-bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}